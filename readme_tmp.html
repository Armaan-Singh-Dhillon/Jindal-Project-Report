<!DOCTYPE html>
<html>
<head>
<title>readme.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="project-theme-optimizing-power-efficiency-in-induction-furnaces-through-advanced-object-detection-technology">Project Theme: Optimizing Power Efficiency in Induction Furnaces through Advanced Object Detection Technology</h1>
<h3 id="name-of-mentor-ahmad-raza">Name of Mentor: Ahmad Raza</h3>
<h3 id="department-sms-2">Department: SMS 2</h3>
<h3 id="trainee-armaan-nitin">Trainee: Armaan, Nitin</h3>
<hr>
<h2 id="context">Context</h2>
<p>In a steel manufacturing process, a ladle is used to transfer molten steel from an Electric Arc Furnace (EAF) to an Argon Oxygen Decarburization (AOD) unit for further chemical processing. The continuous operation and synchronization between these units are critical for maintaining efficiency and product quality. However, there can be instances where the ladle may pause during the transfer process, causing potential delays and disruptions.</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>The goal is to develop a system that can:</p>
<ol>
<li>Detect the ladle as it moves between the EAF and AOD units.</li>
<li>Track the ladle's movement to determine if it is moving or paused.</li>
<li>Provide a signal or alert to the operator when the ladle is paused, so appropriate actions can be taken to minimizeÂ delays.</li>
</ol>
<h2 id="1-understanding-the-project">1. Understanding the Project</h2>
<p>The project involves the application of computer vision techniques to a specific domain. The primary objective is to leverage computer vision technology to solve a problem or improve a process. To gain a thorough understanding of the project's requirements and objectives, a comprehensive project brief was reviewed. This included the project's scope, goals, deliverables, and the expected impact of the project outcomes. The initial phase focused on identifying the specific problem to be addressed, the data required, and the methodologies to be employed in the project.</p>
<h2 id="2-site-visit">2. Site Visit</h2>
<p>A site visit was conducted to gather firsthand information and insights into the practical aspects of the project. During the site visit, observations were made regarding the environment and any potential challenges that might be encountered.This step was crucial in aligning the project's objectives with real-world scenarios.</p>
<h2 id="3-learned-basics-of-computer-vision">3. Learned Basics of Computer Vision</h2>
<p>To effectively contribute to the project, it was essential to acquire a foundational understanding of computer vision. This involved studying the basic concepts and techniques used in the field. Key topics covered included:</p>
<ul>
<li><strong>Image Processing:</strong> Understanding how images are represented, processed, and manipulated in a digital format.</li>
<li><strong>Feature Extraction:</strong> Learning how to identify and extract significant features from images that can be used for analysis.</li>
<li><strong>Object Detection and Recognition:</strong> Studying methods for detecting and recognizing objects within images.</li>
<li><strong>Machine Learning in Computer Vision:</strong> Exploring how machine learning algorithms are applied to computer vision tasks.</li>
</ul>
<p>This foundational knowledge equipped me with the skills needed to approach the project's computer vision tasks competently.</p>
<h2 id="4-installation-of-anaconda">4. Installation of Anaconda</h2>
<p>To create a suitable development environment, Anaconda was installed. Anaconda is a widely-used distribution of Python and R for scientific computing, which simplifies package management and deployment. The steps involved in the installation were:</p>
<ul>
<li>Downloading the Anaconda installer from the official website.</li>
<li>Running the installer and following the on-screen instructions to complete the installation.</li>
<li>Verifying the installation by opening Anaconda Navigator and ensuring that the necessary tools were available.</li>
</ul>
<p>Anaconda provided a robust platform for managing the various packages and dependencies required for the project.</p>
<h2 id="5-creation-of-virtual-environment">5. Creation of Virtual Environment</h2>
<p>A virtual environment was created to ensure that the project's dependencies were isolated and managed efficiently. This involved the following steps:</p>
<ul>
<li>Opening Anaconda Prompt or Terminal.</li>
<li>Creating a new virtual environment using the command: <code>conda create --name project_env python=3.8</code></li>
<li>Activating the virtual environment with the command: <code>conda activate project_env</code></li>
</ul>
<p>The virtual environment allowed for the installation of specific versions of libraries and tools without affecting other projects or the system's global Python environment.</p>
<h2 id="6-installation-of-jupyter-notebook">6. Installation of Jupyter Notebook</h2>
<p>Jupyter Notebook was installed within the virtual environment to facilitate interactive computing and documentation. The steps included:</p>
<ul>
<li>Activating the virtual environment.</li>
<li>Installing Jupyter Notebook using the command: <code>conda install jupyter</code></li>
<li>Launching Jupyter Notebook with the command: <code>jupyter notebook</code></li>
</ul>
<p>Jupyter Notebook provided an interactive platform for developing and testing code, visualizing data, and documenting the analysis process.</p>
<h2 id="7-learned-about-image-labeling-and-annotation">7. Learned About Image Labeling and Annotation</h2>
<p>An essential part of the project involved learning how to label and annotate images. Image labeling and annotation are critical steps in preparing data for computer vision tasks, such as training machine learning models. The process included:</p>
<ul>
<li>Understanding different types of annotations, such as bounding boxes, polygons, and key points.</li>
<li>Using annotation tools like LabelImg and VGG Image Annotator (VIA) to label images.</li>
<li>Creating and managing annotation files in formats such as XML, JSON, or CSV, which are compatible with machine learning frameworks.</li>
</ul>
<p>This knowledge enabled the accurate and consistent labeling of images, which is crucial for the success of the computer vision models.</p>
<hr>
<p>By following these steps, a strong foundation was established for the successful execution of the project. Each phase contributed to a deeper understanding and more effective application of computer vision techniques, ensuring that the project objectives could be met efficiently.</p>
<h2 id="docker">Docker</h2>
<p>Docker is an open-source platform designed to automate the deployment, scaling, and management of applications within containers. Containers are lightweight, stand-alone, executable packages that include everything needed to run a piece of software, including the code, runtime, libraries, and system tools. Docker ensures that applications run consistently across different computing environments.</p>
<h4 id="key-features-of-docker">Key Features of Docker</h4>
<ul>
<li><strong>Portability</strong>: Docker containers can run on any system that supports Docker, ensuring consistent environments across development, testing, and production.</li>
<li><strong>Isolation</strong>: Each container runs in its own isolated environment, preventing conflicts between applications and improving security.</li>
<li><strong>Efficiency</strong>: Containers share the host systemâs kernel and resources, making them more lightweight than traditional virtual machines.</li>
<li><strong>Scalability</strong>: Docker makes it easy to scale applications horizontally by adding more container instances.</li>
</ul>
<h4 id="docker-components">Docker Components</h4>
<ul>
<li><strong>Docker Engine</strong>: The core of Docker, responsible for building and running Docker containers.</li>
<li><strong>Docker Hub</strong>: A cloud-based registry service for sharing container images.</li>
<li><strong>Docker Compose</strong>: A tool for defining and running multi-container Docker applications using a simple YAML file.</li>
</ul>
<h2 id="about-tensorflow">About TensorFlow</h2>
<p>TensorFlow is an open-source machine learning framework developed by the Google Brain team. It is designed to simplify the development and deployment of machine learning models and provides a comprehensive ecosystem of tools, libraries, and community resources.</p>
<h4 id="key-features-of-tensorflow">Key Features of TensorFlow</h4>
<ul>
<li><strong>Flexibility</strong>: TensorFlow supports various machine learning tasks, including neural networks, natural language processing, and computer vision.</li>
<li><strong>Ecosystem</strong>: TensorFlow offers a rich set of tools like TensorBoard for visualization, TensorFlow Lite for mobile and embedded devices, and TensorFlow Extended (TFX) for end-to-end ML pipelines.</li>
<li><strong>Community Support</strong>: TensorFlow has a large and active community, contributing to a wide range of resources, tutorials, and third-party libraries.</li>
<li><strong>Performance</strong>: TensorFlow is optimized for high performance on both CPUs and GPUs, making it suitable for large-scale machine learning tasks.</li>
</ul>
<h4 id="tensorflow-components">TensorFlow Components</h4>
<ul>
<li><strong>TensorFlow Core</strong>: The primary library for building and training machine learning models.</li>
<li><strong>Keras</strong>: An API built on top of TensorFlow for simplifying the creation of neural networks.</li>
<li><strong>TensorFlow Hub</strong>: A repository of reusable machine learning modules.</li>
<li><strong>TensorFlow Serving</strong>: A flexible, high-performance serving system for machine learning models.</li>
</ul>
<p>By leveraging Docker to run TensorFlow, developers can ensure a consistent and isolated environment that simplifies the setup and deployment of machine learning applications.</p>
<h1 id="installing-tensorflow-using-docker">Installing TensorFlow Using Docker</h1>
<h3 id="introduction">Introduction</h3>
<p>This report outlines the steps and considerations for installing TensorFlow in a Docker environment. Docker provides a consistent and isolated environment for running applications, making it an excellent choice for setting up TensorFlow for machine learning and deep learning projects.</p>
<h3 id="prerequisites">Prerequisites</h3>
<p>Before starting, ensure you have the following installed on your system:</p>
<ul>
<li>Docker</li>
<li>Docker Compose (optional, for multi-container setups)</li>
</ul>
<h3 id="steps-for-installation">Steps for Installation</h3>
<ol>
<li>
<p><strong>Pull TensorFlow Docker Image</strong></p>
<p>TensorFlow provides pre-built Docker images for CPU and GPU versions. To pull the latest TensorFlow image for CPU, use the following command:</p>
</li>
</ol>
<pre class="hljs"><code><div>docker pull tensorflow/tensorflow:latest-gpu-jupyter

</div></code></pre>
<ol start="2">
<li><strong>Accessing Jupyter Notebook</strong></li>
</ol>
<p>The TensorFlow Docker image includes Jupyter Notebook. To start a container with Jupyter Notebook, run:</p>
<pre class="hljs"><code><div>
docker run -it --rm -p <span class="hljs-number">8888</span>:<span class="hljs-number">8888</span> tensorflow/tensorflow:latest-jupyter
</div></code></pre>
<h2 id="labelimg">LabelImg</h2>
<p>LabelImg is a graphical image annotation tool used for labeling objects in images for the purpose of training machine learning models, particularly in object detection tasks. It is an open-source tool written in Python and uses the Qt framework for its graphical interface.</p>
<h4 id="key-features-of-labelimg">Key Features of LabelImg</h4>
<ul>
<li><strong>User-Friendly Interface</strong>: Provides an intuitive GUI for easy and efficient image annotation.</li>
<li><strong>Support for Multiple Formats</strong>: Allows saving annotations in both PASCAL VOC and YOLO format, which are widely used in various object detection frameworks.</li>
<li><strong>Cross-Platform</strong>: Available for Windows, macOS, and Linux, making it accessible to a broad range of users.</li>
<li><strong>Customizable Classes</strong>: Users can define custom object classes for labeling, providing flexibility to cater to different project requirements.</li>
</ul>
<h4 id="steps-to-use-labelimg">Steps to Use LabelImg</h4>
<p><strong>Installation</strong></p>
<p>You can install LabelImg via pip or directly from the GitHub repository. To install via pip, use the following command:</p>
<pre class="hljs"><code><div><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(LABELIMG_PATH):
    !mkdir {LABELIMG_PATH}
    !git clone https://github.com/tzutalin/labelImg {LABELIMG_PATH}
<span class="hljs-keyword">if</span> os.name == <span class="hljs-string">'posix'</span>:
    !make qt5py3
<span class="hljs-keyword">if</span> os.name ==<span class="hljs-string">'nt'</span>:
    !cd {LABELIMG_PATH} &amp;&amp; pyrcc5 -o libs/resources.py resources.qrc


</div></code></pre>
<pre class="hljs"><code><div>!<span class="hljs-built_in">cd</span> {LABELIMG_PATH} &amp;&amp; python labelImg.py
</div></code></pre>
<p><img src="file:///c:/Users/armaan/Desktop/New folder (2)/image.png" alt="alt text">
<img src="file:///c:/Users/armaan/Desktop/New folder (2)/image-1.png" alt="alt text"></p>
<hr>
<h3 id="xml-files-an-overview">XML Files: An Overview</h3>
<p><strong>XML (Extensible Markup Language)</strong> is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. XML is designed to store and transport data, and it provides a flexible way to create information formats and share both the format and the data on the World Wide Web, intranets, and elsewhere.</p>
<h3 id="use-of-xml-in-annotating-and-labeling-images">Use of XML in Annotating and Labeling Images</h3>
<p>In the context of machine learning, particularly in computer vision, XML files are commonly used for annotating and labeling images. These annotations are crucial for training models to recognize and classify objects within images.</p>
<h4 id="common-formats-and-applications">Common Formats and Applications:</h4>
<ol>
<li>
<p><strong>Pascal VOC (Visual Object Classes):</strong></p>
<ul>
<li>A widely used format for object detection tasks.</li>
<li>Each image has an associated XML file containing annotations.</li>
<li>The structure includes:
<ul>
<li><strong>folder:</strong> The folder containing the image.</li>
<li><strong>filename:</strong> The name of the image file.</li>
<li><strong>size:</strong> Image dimensions (width, height, depth).</li>
<li><strong>object:</strong> For each object, it includes:
<ul>
<li><strong>name:</strong> Class label of the object.</li>
<li><strong>pose:</strong> (Optional) The pose of the object.</li>
<li><strong>truncated:</strong> Whether the object is truncated.</li>
<li><strong>difficult:</strong> Whether the object is difficult to detect.</li>
<li><strong>bndbox:</strong> The bounding box coordinates (xmin, ymin, xmax, ymax).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>ImageNet:</strong></p>
</li>
</ol>
<p>This format is similar to Pascal VOC but tailored to the ImageNet dataset specifics.</p>
<h4 id="example-of-an-xml-annotation-file-pascal-voc">Example of an XML Annotation File (Pascal VOC):</h4>
<pre class="hljs"><code><div><span class="hljs-tag">&lt;<span class="hljs-name">annotation</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">folder</span>&gt;</span>images<span class="hljs-tag">&lt;/<span class="hljs-name">folder</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">filename</span>&gt;</span>image1.jpg<span class="hljs-tag">&lt;/<span class="hljs-name">filename</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">size</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">width</span>&gt;</span>800<span class="hljs-tag">&lt;/<span class="hljs-name">width</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">height</span>&gt;</span>600<span class="hljs-tag">&lt;/<span class="hljs-name">height</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">depth</span>&gt;</span>3<span class="hljs-tag">&lt;/<span class="hljs-name">depth</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">size</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">object</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>cat<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">pose</span>&gt;</span>Left<span class="hljs-tag">&lt;/<span class="hljs-name">pose</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">truncated</span>&gt;</span>0<span class="hljs-tag">&lt;/<span class="hljs-name">truncated</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">difficult</span>&gt;</span>0<span class="hljs-tag">&lt;/<span class="hljs-name">difficult</span>&gt;</span>
        <span class="hljs-tag">&lt;<span class="hljs-name">bndbox</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-name">xmin</span>&gt;</span>120<span class="hljs-tag">&lt;/<span class="hljs-name">xmin</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-name">ymin</span>&gt;</span>160<span class="hljs-tag">&lt;/<span class="hljs-name">ymin</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-name">xmax</span>&gt;</span>360<span class="hljs-tag">&lt;/<span class="hljs-name">xmax</span>&gt;</span>
            <span class="hljs-tag">&lt;<span class="hljs-name">ymax</span>&gt;</span>480<span class="hljs-tag">&lt;/<span class="hljs-name">ymax</span>&gt;</span>
        <span class="hljs-tag">&lt;/<span class="hljs-name">bndbox</span>&gt;</span>
    <span class="hljs-tag">&lt;/<span class="hljs-name">object</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">annotation</span>&gt;</span>
</div></code></pre>
<p>XML files play a crucial role in the annotation and labeling of images for training machine learning models, particularly in the field of computer vision. Their structured and extensible nature makes them ideal for storing detailed information about image annotations, enabling effective and efficient training of models.</p>
<h1 id="object-detection-model-training-report">Object Detection Model Training Report</h1>
<h2 id="introduction">Introduction</h2>
<p>This report documents the process of training a custom object detection model using TensorFlow Object Detection API. The objective is to create a model capable of detecting specific objects in images and real-time video streams.</p>
<h2 id="methodology">Methodology</h2>
<h3 id="1-setting-up-paths-and-configurations">1. Setting up Paths and Configurations</h3>
<ul>
<li>Define paths and configurations for the project</li>
<li>This includes paths for workspace, scripts, pretrained models, etc.</li>
<li>Explanation: This section sets up necessary paths and configurations required for the project.</li>
<li>These paths are used throughout the training process for storing data scripts and models</li>
</ul>
<pre class="hljs"><code><div>CUSTOM_MODEL_NAME = <span class="hljs-string">'my_ssd_mobnet'</span>
PRETRAINED_MODEL_NAME = <span class="hljs-string">'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'</span>
PRETRAINED_MODEL_URL = <span class="hljs-string">'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'</span>
TF_RECORD_SCRIPT_NAME = <span class="hljs-string">'generate_tfrecord.py'</span>
LABEL_MAP_NAME = <span class="hljs-string">'label_map.pbtxt'</span>

paths = {
    <span class="hljs-string">'WORKSPACE_PATH'</span>: os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>),
    <span class="hljs-string">'SCRIPTS_PATH'</span>: os.path.join(<span class="hljs-string">'Tensorflow'</span>,<span class="hljs-string">'scripts'</span>),
    <span class="hljs-string">'APIMODEL_PATH'</span>: os.path.join(<span class="hljs-string">'Tensorflow'</span>,<span class="hljs-string">'models'</span>),
    <span class="hljs-string">'ANNOTATION_PATH'</span>: os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>,<span class="hljs-string">'annotations'</span>),
    <span class="hljs-string">'IMAGE_PATH'</span>: os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>,<span class="hljs-string">'images'</span>),
    <span class="hljs-string">'MODEL_PATH'</span>: os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>,<span class="hljs-string">'models'</span>),
    <span class="hljs-string">'PRETRAINED_MODEL_PATH'</span>: os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>,<span class="hljs-string">'pre-trained-models'</span>),
    <span class="hljs-string">'CHECKPOINT_PATH'</span>: os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>,<span class="hljs-string">'models'</span>,CUSTOM_MODEL_NAME),
    <span class="hljs-string">'OUTPUT_PATH'</span>: os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>,<span class="hljs-string">'models'</span>,CUSTOM_MODEL_NAME, <span class="hljs-string">'export'</span>),
    <span class="hljs-string">'TFJS_PATH'</span>:os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>,<span class="hljs-string">'models'</span>,CUSTOM_MODEL_NAME, <span class="hljs-string">'tfjsexport'</span>),
    <span class="hljs-string">'TFLITE_PATH'</span>:os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>,<span class="hljs-string">'models'</span>,CUSTOM_MODEL_NAME, <span class="hljs-string">'tfliteexport'</span>),
    <span class="hljs-string">'PROTOC_PATH'</span>:os.path.join(<span class="hljs-string">'Tensorflow'</span>,<span class="hljs-string">'protoc'</span>)
 }

files = {
    <span class="hljs-string">'PIPELINE_CONFIG'</span>:os.path.join(<span class="hljs-string">'Tensorflow'</span>, <span class="hljs-string">'workspace'</span>,<span class="hljs-string">'models'</span>, CUSTOM_MODEL_NAME, <span class="hljs-string">'pipeline.config'</span>),
    <span class="hljs-string">'TF_RECORD_SCRIPT'</span>: os.path.join(paths[<span class="hljs-string">'SCRIPTS_PATH'</span>], TF_RECORD_SCRIPT_NAME),
    <span class="hljs-string">'LABELMAP'</span>: os.path.join(paths[<span class="hljs-string">'ANNOTATION_PATH'</span>], LABEL_MAP_NAME)
}
</div></code></pre>
<h3 id="2-downloading-tf-models-pretrained-models-and-installing-tfod">2. Downloading TF Models Pretrained Models and Installing TFOD</h3>
<ul>
<li>Download TensorFlow Models repository and install required packages</li>
<li>Explanation: This section downloads the TensorFlow Models repository if it's not already present.</li>
<li>It also installs necessary packages and sets up the Object Detection API for training.</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-comment"># https://www.tensorflow.org/install/source_windows</span>
<span class="hljs-keyword">import</span> re
!apt-get install wget --yes
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(os.path.join(paths[<span class="hljs-string">'APIMODEL_PATH'</span>], <span class="hljs-string">'research'</span>, <span class="hljs-string">'object_detection'</span>)):
    !git clone https://github.com/tensorflow/models {paths[<span class="hljs-string">'APIMODEL_PATH'</span>]}

<span class="hljs-comment"># Clone the TensorFlow models repository from GitHub</span>
<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(os.path.join(paths[<span class="hljs-string">'APIMODEL_PATH'</span>], <span class="hljs-string">'research'</span>, <span class="hljs-string">'object_detection'</span>)):
    !git clone --depth <span class="hljs-number">1</span> https://github.com/tensorflow/models {paths[<span class="hljs-string">'APIMODEL_PATH'</span>]}
<span class="hljs-comment"># Copy setup files into models/research folder</span>
%%bash
cd Tensorflow/models/research/
protoc object_detection/protos/*.proto --python_out=.
<span class="hljs-comment">#cp object_detection/packages/tf2/setup.py .</span>

<span class="hljs-comment"># Uninstall Cython as a temporary fix for the "No module named 'object_detection'" error</span>
!pip uninstall Cython -y

<span class="hljs-comment"># Modify setup.py file to install the tf-models-official repository targeted at TF v2.8.0</span>
setup_file_path = os.path.join(paths[<span class="hljs-string">'APIMODEL_PATH'</span>], <span class="hljs-string">'research'</span>, <span class="hljs-string">'object_detection'</span>, <span class="hljs-string">'packages'</span>, <span class="hljs-string">'tf2'</span>, <span class="hljs-string">'setup.py'</span>)
<span class="hljs-keyword">with</span> open(setup_file_path) <span class="hljs-keyword">as</span> f:
    s = f.read()

<span class="hljs-keyword">with</span> open(os.path.join(paths[<span class="hljs-string">'APIMODEL_PATH'</span>], <span class="hljs-string">'research'</span>, <span class="hljs-string">'setup.py'</span>), <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:
    s = re.sub(<span class="hljs-string">'tf-models-official&gt;=2.5.1'</span>, <span class="hljs-string">'tf-models-official==2.8.0'</span>, s)
    f.write(s)

<span class="hljs-comment"># Install the Object Detection API (<span class="hljs-doctag">NOTE:</span> This block takes about 10 minutes to finish executing)</span>
!pip install pyyaml==<span class="hljs-number">5.3</span>
!pip install {os.path.join(paths[<span class="hljs-string">'APIMODEL_PATH'</span>], <span class="hljs-string">'research'</span>)}

<span class="hljs-comment"># Need to downgrade to TF v2.8.0 due to Colab compatibility bug with TF v2.10 (as of 10/03/22)</span>
!pip install tensorflow==<span class="hljs-number">2.8</span><span class="hljs-number">.0</span>

!pip install tensorflow_io==<span class="hljs-number">0.23</span><span class="hljs-number">.1</span>

VERIFICATION_SCRIPT = os.path.join(paths[<span class="hljs-string">'APIMODEL_PATH'</span>], <span class="hljs-string">'research'</span>, <span class="hljs-string">'object_detection'</span>, <span class="hljs-string">'builders'</span>, <span class="hljs-string">'model_builder_tf2_test.py'</span>)
<span class="hljs-comment"># Verify Installation</span>
!python {VERIFICATION_SCRIPT}

!pip install tensorflow --upgrade

!pip uninstall protobuf matplotlib -y
!pip install protobuf matplotlib==<span class="hljs-number">3.2</span>

<span class="hljs-keyword">import</span> object_detection

<span class="hljs-keyword">if</span> os.name ==<span class="hljs-string">'posix'</span>:
    !wget {PRETRAINED_MODEL_URL}
    !mv {PRETRAINED_MODEL_NAME+<span class="hljs-string">'.tar.gz'</span>} {paths[<span class="hljs-string">'PRETRAINED_MODEL_PATH'</span>]}
    !cd {paths[<span class="hljs-string">'PRETRAINED_MODEL_PATH'</span>]} &amp;&amp; tar -zxvf {PRETRAINED_MODEL_NAME+<span class="hljs-string">'.tar.gz'</span>}
<span class="hljs-keyword">if</span> os.name == <span class="hljs-string">'nt'</span>:
    wget.download(PRETRAINED_MODEL_URL)
    !move {PRETRAINED_MODEL_NAME+<span class="hljs-string">'.tar.gz'</span>} {paths[<span class="hljs-string">'PRETRAINED_MODEL_PATH'</span>]}
    !cd {paths[<span class="hljs-string">'PRETRAINED_MODEL_PATH'</span>]} &amp;&amp; tar -zxvf {PRETRAINED_MODEL_NAME+<span class="hljs-string">'.tar.gz'</span>}


</div></code></pre>
<h3 id="3-creating-label-map">3. Creating Label Map</h3>
<ul>
<li>Create label map for custom objects</li>
<li>Explanation: Label maps associate class names with unique IDs.</li>
<li>In this section, a label map is created for the custom objects to be detected by the model.</li>
</ul>
<pre class="hljs"><code><div>

labels = [{<span class="hljs-string">'name'</span>:<span class="hljs-string">'ThumbsUp'</span>, <span class="hljs-string">'id'</span>:<span class="hljs-number">1</span>}, {<span class="hljs-string">'name'</span>:<span class="hljs-string">'ThumbsDown'</span>, <span class="hljs-string">'id'</span>:<span class="hljs-number">2</span>}, {<span class="hljs-string">'name'</span>:<span class="hljs-string">'ThankYou'</span>, <span class="hljs-string">'id'</span>:<span class="hljs-number">3</span>}, {<span class="hljs-string">'name'</span>:<span class="hljs-string">'LiveLong'</span>, <span class="hljs-string">'id'</span>:<span class="hljs-number">4</span>}]

<span class="hljs-keyword">with</span> open(files[<span class="hljs-string">'LABELMAP'</span>], <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:
    <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels:
        f.write(<span class="hljs-string">'item { \n'</span>)
        f.write(<span class="hljs-string">'\tname:\'{}\'\n'</span>.format(label[<span class="hljs-string">'name'</span>]))
        f.write(<span class="hljs-string">'\tid:{}\n'</span>.format(label[<span class="hljs-string">'id'</span>]))
        f.write(<span class="hljs-string">'}\n'</span>)
</div></code></pre>
<h3 id="4-creating-tf-records">4. Creating TF Records</h3>
<ul>
<li>Convert dataset into TFRecord format</li>
<li>Explanation: TFRecord files store the dataset in a format suitable for TensorFlow.</li>
<li>This step is crucial for training the model as TensorFlow expects data in TFRecord format.</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-comment"># OPTIONAL IF RUNNING ON COLAB</span>
ARCHIVE_FILES = os.path.join(paths[<span class="hljs-string">'IMAGE_PATH'</span>], <span class="hljs-string">'archive.tar.gz'</span>)
<span class="hljs-keyword">if</span> os.path.exists(ARCHIVE_FILES):
  !tar -zxvf {ARCHIVE_FILES}

<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(files[<span class="hljs-string">'TF_RECORD_SCRIPT'</span>]):
    !git clone https://github.com/nicknochnack/GenerateTFRecord {paths[<span class="hljs-string">'SCRIPTS_PATH'</span>]}

!python {files[<span class="hljs-string">'TF_RECORD_SCRIPT'</span>]} -x {os.path.join(paths[<span class="hljs-string">'IMAGE_PATH'</span>], <span class="hljs-string">'train'</span>)} -l {files[<span class="hljs-string">'LABELMAP'</span>]} -o {os.path.join(paths[<span class="hljs-string">'ANNOTATION_PATH'</span>], <span class="hljs-string">'train.record'</span>)}
!python {files[<span class="hljs-string">'TF_RECORD_SCRIPT'</span>]} -x {os.path.join(paths[<span class="hljs-string">'IMAGE_PATH'</span>], <span class="hljs-string">'test'</span>)} -l {files[<span class="hljs-string">'LABELMAP'</span>]} -o {os.path.join(paths[<span class="hljs-string">'ANNOTATION_PATH'</span>], <span class="hljs-string">'test.record'</span>)}


</div></code></pre>
<h3 id="5-copying-model-config-to-training-folder">5. Copying Model Config to Training Folder</h3>
<ul>
<li>Copy pretrained model configuration to the custom model directory</li>
<li>Explanation: The pipeline configuration file from the pretrained model is copied to the custom model directory.</li>
<li>This configuration file specifies the architecture and parameters of the model.</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-keyword">if</span> os.name ==<span class="hljs-string">'posix'</span>:
    !cp {os.path.join(paths[<span class="hljs-string">'PRETRAINED_MODEL_PATH'</span>], PRETRAINED_MODEL_NAME, <span class="hljs-string">'pipeline.config'</span>)} {os.path.join(paths[<span class="hljs-string">'CHECKPOINT_PATH'</span>])}
<span class="hljs-keyword">if</span> os.name == <span class="hljs-string">'nt'</span>:
    !copy {os.path.join(paths[<span class="hljs-string">'PRETRAINED_MODEL_PATH'</span>], PRETRAINED_MODEL_NAME, <span class="hljs-string">'pipeline.config'</span>)} {os.path.join(paths[<span class="hljs-string">'CHECKPOINT_PATH'</span>])}
</div></code></pre>
<h3 id="6-updating-config-for-transfer-learning">6. Updating Config For Transfer Learning</h3>
<ul>
<li>Update pipeline configuration for transfer learning</li>
<li>Explanation: The pipeline configuration is updated with details relevant to the custom model.</li>
<li>This includes the number of classes, batch size, fine-tuning checkpoint, etc.</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> object_detection.utils <span class="hljs-keyword">import</span> config_util
<span class="hljs-keyword">from</span> object_detection.protos <span class="hljs-keyword">import</span> pipeline_pb2
<span class="hljs-keyword">from</span> google.protobuf <span class="hljs-keyword">import</span> text_format

config = config_util.get_configs_from_pipeline_file(files[<span class="hljs-string">'PIPELINE_CONFIG'</span>])

config

pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
<span class="hljs-keyword">with</span> tf.io.gfile.GFile(files[<span class="hljs-string">'PIPELINE_CONFIG'</span>], <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> f:
    proto_str = f.read()
    text_format.Merge(proto_str, pipeline_config)

pipeline_config.model.ssd.num_classes = len(labels)
pipeline_config.train_config.batch_size = <span class="hljs-number">4</span>
pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths[<span class="hljs-string">'PRETRAINED_MODEL_PATH'</span>], PRETRAINED_MODEL_NAME, <span class="hljs-string">'checkpoint'</span>, <span class="hljs-string">'ckpt-0'</span>)
pipeline_config.train_config.fine_tune_checkpoint_type = <span class="hljs-string">"detection"</span>
pipeline_config.train_input_reader.label_map_path= files[<span class="hljs-string">'LABELMAP'</span>]
pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths[<span class="hljs-string">'ANNOTATION_PATH'</span>], <span class="hljs-string">'train.record'</span>)]
pipeline_config.eval_input_reader[<span class="hljs-number">0</span>].label_map_path = files[<span class="hljs-string">'LABELMAP'</span>]
pipeline_config.eval_input_reader[<span class="hljs-number">0</span>].tf_record_input_reader.input_path[:] = [os.path.join(paths[<span class="hljs-string">'ANNOTATION_PATH'</span>], <span class="hljs-string">'test.record'</span>)]

config_text = text_format.MessageToString(pipeline_config)
<span class="hljs-keyword">with</span> tf.io.gfile.GFile(files[<span class="hljs-string">'PIPELINE_CONFIG'</span>], <span class="hljs-string">"wb"</span>) <span class="hljs-keyword">as</span> f:
    f.write(config_text)


</div></code></pre>
<h3 id="7-training-the-model">7. Training the Model</h3>
<ul>
<li>Train the custom object detection model</li>
<li>Explanation: The model is trained using the specified configuration and dataset.</li>
<li>This step involves running the training script with appropriate parameters.</li>
</ul>
<pre class="hljs"><code><div>
TRAINING_SCRIPT = os.path.join(paths[<span class="hljs-string">'APIMODEL_PATH'</span>], <span class="hljs-string">'research'</span>, <span class="hljs-string">'object_detection'</span>, <span class="hljs-string">'model_main_tf2.py'</span>)

command = <span class="hljs-string">"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=3000"</span>.format(TRAINING_SCRIPT, paths[<span class="hljs-string">'CHECKPOINT_PATH'</span>],files[<span class="hljs-string">'PIPELINE_CONFIG'</span>])
!{command}
</div></code></pre>
<h3 id="loss-regulation">Loss Regulation</h3>
<p><img src="file:///c:/Users/armaan/Desktop/New folder (2)/image-3.png" alt="alt text"></p>
<h3 id="9-loading-trained-model-from-checkpoint">9. Loading Trained Model From Checkpoint</h3>
<ul>
<li>Load trained model from checkpoint</li>
<li>Explanation: The trained model is loaded from the checkpoint for further use.</li>
<li>This allows us to perform inference or continue training from the saved checkpoint.</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline

category_index = label_map_util.create_category_index_from_labelmap(files[<span class="hljs-string">'LABELMAP'</span>])

IMAGE_PATH = os.path.join(paths[<span class="hljs-string">'IMAGE_PATH'</span>], <span class="hljs-string">'test'</span>, <span class="hljs-string">'thankyou.54820f05-2498-11ef-9aa8-4485002d90ff.jpg'</span>)

img = cv2.imread(IMAGE_PATH)
image_np = np.array(img)

input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, <span class="hljs-number">0</span>), dtype=tf.float32)
detections = detect_fn(input_tensor)

num_detections = int(detections.pop(<span class="hljs-string">'num_detections'</span>))
detections = {key: value[<span class="hljs-number">0</span>, :num_detections].numpy()
              <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> detections.items()}
detections[<span class="hljs-string">'num_detections'</span>] = num_detections

<span class="hljs-comment"># detection_classes should be ints.</span>
detections[<span class="hljs-string">'detection_classes'</span>] = detections[<span class="hljs-string">'detection_classes'</span>].astype(np.int64)

label_id_offset = <span class="hljs-number">1</span>
image_np_with_detections = image_np.copy()

viz_utils.visualize_boxes_and_labels_on_image_array(
            image_np_with_detections,
            detections[<span class="hljs-string">'detection_boxes'</span>],
            detections[<span class="hljs-string">'detection_classes'</span>]+label_id_offset,
            detections[<span class="hljs-string">'detection_scores'</span>],
            category_index,
            use_normalized_coordinates=<span class="hljs-literal">True</span>,
            max_boxes_to_draw=<span class="hljs-number">5</span>,
            min_score_thresh=<span class="hljs-number">.8</span>,
            agnostic_mode=<span class="hljs-literal">False</span>)

plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))
plt.show()

</div></code></pre>
<p><img src="file:///c:/Users/armaan/Desktop/New folder (2)/image-4.png" alt="alt text"></p>

</body>
</html>
